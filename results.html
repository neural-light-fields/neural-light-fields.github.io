<!doctype html>
<html>
<head>
<title>Learning Neural Light Fields with Ray-Space Embedding Networks | Supplemental Results</title>
<link href="resources/style.css" rel="stylesheet"/>
<script src="resources/handlers.js"></script>
<script>
window.onload = function() {
  imgcmp_sparse_handler = new ImageHandler(8, 4, "imgcmp_sparse_data", "imgcmp_sparse_btn", "imgcmp_sparse_img");
  imgcmp_sparse_handler.register();

  imgcmp_dense_handler = new ImageHandler(12, 4, "imgcmp_dense_data", "imgcmp_dense_btn", "imgcmp_dense_img");
  imgcmp_dense_handler.register();

  imgabl_embedding_handler = new ImageHandler(20, 4, "imgabl_embedding_data", "imgabl_embedding_btn", "imgabl_embedding_img");
  imgabl_embedding_handler.register();

  imgabl_subdivision_handler = new ImageHandler(8, 5, "imgabl_subdivision_data", "imgabl_subdivision_btn", "imgabl_subdivision_img");
  imgabl_subdivision_handler.register();

  vidcmp_sparse_handler = new VideoHandler(3, 3, "vidcmp_sparse_data", "vidcmp_sparse_btn", "vidcmp_sparse_vid");
  vidcmp_sparse_handler.register();
  vidcmp_sparse_handler.play_video();

  vidcmp_dense_handler = new VideoHandler(3, 4, "vidcmp_dense_data", "vidcmp_dense_btn", "vidcmp_dense_vid");
  vidcmp_dense_handler.register();
  vidcmp_dense_handler.play_video();

  vidcmp_embed_handler = new VideoHandler(1, 4, "vidcmp_embed_data", "vidcmp_embed_btn", "vidcmp_embed_vid");
  vidcmp_embed_handler.register();
  vidcmp_embed_handler.play_video();

  imgcmp_shiny_handler = new ImageHandler(2, 4, "imgcmp_shiny_data", "imgcmp_shiny_btn", "imgcmp_shiny_img");
  imgcmp_shiny_handler.register();
};
</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CQVT3LMJLZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-CQVT3LMJLZ');
</script>
</head>
<body>
<a href="index.html"><h1 id="top" class="center">Learning Neural Light Fields with Ray-Space Embedding Networks</h1></a>
<h2 class="center">Supplemental Results</h2>
<div class="authors">
  <div>
    <p><a href="https://www.battal.me/">Benjamin Attal</a></p>
    <p>Carnegie Mellon University</p>
  </div>
  <div>
    <p><a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a></p>
    <p>Meta</p>
  </div>
  <div>
    <p><a href="https://zollhoefer.com/">Michael Zollh&ouml;fer</a></p>
    <p>Reality Labs Research</p>
  </div>
  <div>
    <p><a href="https://johanneskopf.de/">Johannes Kopf</a></p>
    <p>Meta</p>
  </div>
  <div>
    <p><a href="https://changilkim.com/">Changil Kim</a></p>
    <p>Meta</p>
  </div>
</div>


<h2>Table of Contents</h2>
<div>
  <ul>
    <li>Comparisons with Baselines
      <ul>
        <li><a href="#imgcmp_sparse">Sparse Light Fields</a></li>
        <li><a href="#imgcmp_dense">Dense Light Fields</a></li>
      </ul>
    </li>
    <li>Ablations
      <ul>
        <li><a href="#imgabl_embedding">Embedding Networks</a></li>
        <li><a href="#imgabl_subdivision">Subdivision</a></li>
      </ul>
    </li>
    <li><a href="#imgcmp_shiny">Qualititative Comparisons on the Dense Shiny Dataset Scenes</a></li>
    <li>Video Comparisons with Baselines
      <ul>
        <li><a href="#vidcmp_sparse">Sparse Light Fields</a></li>
        <li><a href="#vidcmp_dense">Dense Light Fields</a></li>
      </ul>
    </li>
    <li><a href="#vidcmp_embed">Embedding Comparison for Dense Light Fields</a></li>
  </ul>
</div>
<h2 id="imgcmp_sparse">
  Comparisons on Sparse Light Fields
  <span><a href="#top">&uarr;</a></span>
</h2>
<div class="flex">
  <div>
    <div class="flex">
      <div>
        <button id="imgcmp_sparse_data0" value="llff_fern">Fern</button>
        <button id="imgcmp_sparse_data1" value="llff_flower">Flower</button>
        <button id="imgcmp_sparse_data2" value="llff_fortress">Fortress</button>
        <button id="imgcmp_sparse_data3" value="llff_horns" class="on">Horns</button>
        <button id="imgcmp_sparse_data4" value="llff_leaves">Leaves</button>
        <button id="imgcmp_sparse_data5" value="llff_orchids">Orchids</button>
        <button id="imgcmp_sparse_data6" value="llff_room">Room</button>
        <button id="imgcmp_sparse_data7" value="llff_trex">T-Rex</button>
      </div>
      <div class="wider_buttons">
        <button id="imgcmp_sparse_btn0" value="gt">Ground Truth</button>
        <button id="imgcmp_sparse_btn1" value="ours" class="on">Our Result</button>
        <button id="imgcmp_sparse_btn2" value="nerf">NeRF</button>
        <button id="imgcmp_sparse_btn3" value="ours_teacher">Ours (w/ Teacher)</button>
      </div>
    </div>
    <div class="desc">
      <p>We evaluate our model with a 32<sup>3</sup> voxel grid on sparse, unstructured light fields.</p>
      <p><b>Datasets.</b> We use all 8 scenes from the NeRF Real Forward-Facing dataset.</p>
      <p><b>Baselines.</b> We observe better reproduction of view dependence, sharpness, and overall improved perceptual fidelity of our method over NeRF, while rendering much faster. We can sometimes struggle when there exist thin features with large disparity and complex occlusions (see the Fern sequence).</p>
    </div>
  </div>
  <div>
    <img id="imgcmp_sparse_img0" src="images/llff_horns_gt.png"/>
    <img id="imgcmp_sparse_img1" src="images/llff_horns_ours.png" class="on"/>
    <img id="imgcmp_sparse_img2" src="images/llff_horns_nerf.png"/>
    <img id="imgcmp_sparse_img3" src="images/llff_horns_ours_teacher.png"/>
  </div>
</div>
<h2 id="imgcmp_dense">
  Comparisons on Dense Light Fields
  <span><a href="#top">&uarr;</a></span>
</h2>
<div class="flex">
  <div>
    <div class="flex">
      <div>
        <button id="imgcmp_dense_data0" value="stanford_gem">Amethyst</button>
        <button id="imgcmp_dense_data1" value="stanford_beans">Beans</button>
        <button id="imgcmp_dense_data2" value="stanford_bracelet">Bracelet</button>
        <button id="imgcmp_dense_data3" value="stanford_bulldozer">Bulldozer</button>
        <button id="imgcmp_dense_data4" value="stanford_bunny">Bunny</button>
        <button id="imgcmp_dense_data5" value="stanford_chess">Chess</button>
        <button id="imgcmp_dense_data6" value="stanford_flowers">Flowers</button>
        <button id="imgcmp_dense_data7" value="stanford_knights">Knights</button>
        <button id="imgcmp_dense_data9" value="stanford_tarot_small" class="on">Tarot (Small)</button>
        <button id="imgcmp_dense_data8" value="stanford_tarot">Tarot (Large)</button>
        <button id="imgcmp_dense_data10" value="stanford_treasure">Treasure</button>
        <button id="imgcmp_dense_data11" value="stanford_truck">Truck</button>
      </div>
      <div class="wider_buttons">
        <button id="imgcmp_dense_btn0" value="gt">Ground Truth</button>
        <button id="imgcmp_dense_btn1" value="ours" class="on">Our Result</button>
        <button id="imgcmp_dense_btn2" value="nerf">NeRF</button>
        <button id="imgcmp_dense_btn3" value="xfields">X-Fields</button>
      </div>
    </div>
    <div class="desc">
      <p>We evaluate our model without subdivision on dense 5x5 light fields.</p>
      <p><b>Datasets.</b> We use all 12 scenes from the Stanford Light Fields dataset.</p>
      <p><b>Baselines.</b> NeRF interpolates in a geometrically consistent manner, but fails to capture high frequency view dependence such as reflections and refractions. X-Fields also struggles with high frequency view dependence in some cases, and can also exhibit minor ghosting artifacts. We render much faster than NeRF, and about half as fast as X-Fields. Both baselines, in general, have worse sharpness than our method&mdash;however, our embedding network struggles to identify correspondences and interpolate on larger baseline sequences such as Tarot (Large) and Knights.</p>
    </div>
  </div>
  <div>
    <img id="imgcmp_dense_img0" src="images/stanford_tarot_small_gt.png"/>
    <img id="imgcmp_dense_img1" src="images/stanford_tarot_small_ours.png" class="on"/>
    <img id="imgcmp_dense_img2" src="images/stanford_tarot_small_nerf.png"/>
    <img id="imgcmp_dense_img3" src="images/stanford_tarot_small_xfields.png"/>
  </div>
</div>
<h2 id="imgabl_embedding">
  Ablations on Embedding Networks
  <span><a href="#top">&uarr;</a></span>
</h2>
<div class="flex">
  <div>
    <div class="flex">
      <div>
        <p class="btncap">Sparse</p>
        <button id="imgabl_embedding_data0" value="llff_fern">Fern</button>
        <button id="imgabl_embedding_data1" value="llff_flower">Flower</button>
        <button id="imgabl_embedding_data2" value="llff_fortress">Fortress</button>
        <button id="imgabl_embedding_data3" value="llff_horns" class="on">Horns</button>
        <button id="imgabl_embedding_data4" value="llff_leaves">Leaves</button>
        <button id="imgabl_embedding_data5" value="llff_orchids">Orchids</button>
        <button id="imgabl_embedding_data6" value="llff_room">Room</button>
        <button id="imgabl_embedding_data7" value="llff_trex">T-Rex</button>
        <p class="btncap">Dense</p>
        <button id="imgabl_embedding_data8" value="stanford_gem">Amethyst</button>
        <button id="imgabl_embedding_data9" value="stanford_beans">Beans</button>
        <button id="imgabl_embedding_data10" value="stanford_bracelet">Bracelet</button>
        <button id="imgabl_embedding_data11" value="stanford_bulldozer">Bulldozer</button>
        <button id="imgabl_embedding_data12" value="stanford_bunny">Bunny</button>
        <button id="imgabl_embedding_data13" value="stanford_chess">Chess</button>
        <button id="imgabl_embedding_data14" value="stanford_flowers">Flowers</button>
        <button id="imgabl_embedding_data15" value="stanford_knights">Knights</button>
        <button id="imgabl_embedding_data16" value="stanford_tarot_small">Tarot (Small)</button>
        <button id="imgabl_embedding_data17" value="stanford_tarot">Tarot (Large)</button>
        <button id="imgabl_embedding_data18" value="stanford_treasure">Treasure</button>
        <button id="imgabl_embedding_data19" value="stanford_truck">Truck</button>
      </div>
      <div class="wider_buttons">
        <button id="imgabl_embedding_btn0" value="gt">Ground Truth</button>
        <button id="imgabl_embedding_btn1" value="ours_no_warp">No Embedding</button>
        <button id="imgabl_embedding_btn2" value="ours_feature">Feature Embedding</button>
        <button id="imgabl_embedding_btn3" value="ours" class="on">Local Affine-Transformation Embedding</button>
      </div>
    </div>
    <div class="desc">
      <p>We evaluate alternative embedding approaches, including our model without an embedding network and our model with feature space embedding. All experiments on sparse data are for our subdivided model with a 32<sup>3</sup> voxel grid, while no subdivision is used for dense data.</p>
      <p><b>Datasets.</b> We use 8 scenes from the NeRF Real Forward-Facing dataset and 12 scenes from the Stanford Light Fields dataset.</p>
      <p><b>Baselines.</b> Local affine-transformation embedding leads to the best overall qualititative and quantitative performance. For some large baseline scenes, such as Tarot (Large), feature embedding can lead to better interpolation of refractive effects, since it does not implicitly enforce local planarity of color level sets within the light field.</p>
    </div>
  </div>
  <div>
    <img id="imgabl_embedding_img0" src="images/llff_horns_gt.png"/>
    <img id="imgabl_embedding_img1" src="images/llff_horns_ours_no_warp.png"/>
    <img id="imgabl_embedding_img2" src="images/llff_horns_ours_feature.png"/>
    <img id="imgabl_embedding_img3" src="images/llff_horns_ours.png" class="on"/>
  </div>
</div>
<h2 id="imgabl_subdivision">
  Ablations on Subdivision
  <span><a href="#top">&uarr;</a></span>
</h2>
<div class="flex">
  <div>
    <div class="flex">
      <div>
        <button id="imgabl_subdivision_data0" value="llff_fern">Fern</button>
        <button id="imgabl_subdivision_data1" value="llff_flower">Flower</button>
        <button id="imgabl_subdivision_data2" value="llff_fortress">Fortress</button>
        <button id="imgabl_subdivision_data3" value="llff_horns" class="on">Horns</button>
        <button id="imgabl_subdivision_data4" value="llff_leaves">Leaves</button>
        <button id="imgabl_subdivision_data5" value="llff_orchids">Orchids</button>
        <button id="imgabl_subdivision_data6" value="llff_room">Room</button>
        <button id="imgabl_subdivision_data7" value="llff_trex">T-Rex</button>
      </div>
      <div class="wider_buttons">
        <button id="imgabl_subdivision_btn0" value="gt">Ground Truth</button>
        <button id="imgabl_subdivision_btn1" value="ours_4">4<sup>3</sup></button>
        <button id="imgabl_subdivision_btn2" value="ours_8">8<sup>3</sup></button>
        <button id="imgabl_subdivision_btn3" value="ours_16">16<sup>3</sup></button>
        <button id="imgabl_subdivision_btn4" value="ours" class="on">32<sup>3</sup></button>
      </div>
    </div>
    <div class="desc">
      <p>We evaluate our subdivided model with different resolution voxel grids, from 4<sup>3</sup> to 32<sup>3</sup>.</p>
      <p><b>Datasets.</b> We use 8 scenes from the NeRF Real Forward-Facing dataset.</p>
      <p><b>Baselines.</b> Even with low resolution voxel grids, our model still produces accurate image predictions, albeit with slightly reduced geometric consistency.</p>
    </div>
  </div>
  <div>
    <img id="imgabl_subdivision_img0" src="images/llff_horns_gt.png"/>
    <img id="imgabl_subdivision_img1" src="images/llff_horns_ours_4.png"/>
    <img id="imgabl_subdivision_img2" src="images/llff_horns_ours_8.png"/>
    <img id="imgabl_subdivision_img3" src="images/llff_horns_ours_16.png"/>
    <img id="imgabl_subdivision_img4" src="images/llff_horns_ours.png" class="on"/>
  </div>
</div>
<h2 id="imgcmp_shiny">
  Qualititative Comparisons on the Dense Shiny Dataset Scenes
  <span><a href="#top">&uarr;</a></span>
</h2>
<div class="flex">
  <div>
    <div class="flex">
      <div>
        <button id="imgcmp_shiny_data0" value="shiny_cd" class="on">CD</button>
        <button id="imgcmp_shiny_data1" value="shiny_lab">Lab</button>
      </div>
      <div class="wider_buttons">
        <button id="imgcmp_shiny_btn0" value="gt">Ground Truth</button>
        <button id="imgcmp_shiny_btn1" value="ours" class="on">Our Result</button>
        <button id="imgcmp_shiny_btn2" value="nerf">NeRF</button>
        <button id="imgcmp_shiny_btn3" value="nex">NeX</button>
      </div>
    </div>
    <div class="desc">
      <p>We show qualitative results on unstructured, dense sequences within the Shiny dataset.</p>
      <p><b>Datasets.</b> We use 2 scenes from the Shiny Dataset&mdash;CD and Lab.</a></p>
      <p><b>Baselines.</b> We observe better reproduction of view dependence for heldout views, specifically of reflections and refractions. Observe the refractions in the liquids in both sequences, and the reflections on the CD and textbook.</p>
    </div>
  </div>
  <div>
    <img id="imgcmp_shiny_img0" src="images/shiny_cd_gt.png" class="shiny_img"/>
    <img id="imgcmp_shiny_img1" src="images/shiny_cd_ours.png" class="shiny_img on"/>
    <img id="imgcmp_shiny_img2" src="images/shiny_cd_nerf.png" class="shiny_img"/>
    <img id="imgcmp_shiny_img3" src="images/shiny_cd_nex.png" class="shiny_img"/>
  </div>
</div>
<h2 id="vidcmp_sparse">
  View Synthesis Videos from Sparse Light Fields
  <span><a href="#top">&uarr;</a></span>
</h2>
<div class="flex">
  <div>
    <div class="flex">
      <div>
        <button id="vidcmp_sparse_data0" value="llff_fern">Fern</button>
        <button id="vidcmp_sparse_data1" value="llff_horns" class="on">Horns</button>
        <button id="vidcmp_sparse_data2" value="llff_leaves">Leaves</button>
      </div>
      <div class="wider_buttons">
        <button id="vidcmp_sparse_btn0" value="ours" class="on">Our Result</button>
        <button id="vidcmp_sparse_btn1" value="nerf">NeRF</button>
        <button id="vidcmp_sparse_btn2" value="ours_teacher">Ours (w/ Teacher)</button>
      </div>
    </div>
    <div class="desc">
      <p>We show spiral trajectories for a select few scenes.</p>
      <p><b>Datasets.</b> We use 3 scenes from the NeRF Real Forward-Facing dataset&mdash;Fern, Horns, and Leaves.</p>
      <p><b>Baselines.</b> For the Fern scene, our model exhbits some artifacts around the fern leaves, which improve with student teacher training. For the Horns scene, we better reproduce the many glass reflections. For the Leaves scene, although we have worse quantitative performance on PSNR and SSIM (and some floating artifacts near the boundaries of the images), our models with and without student teacher training have better sharpness than NeRF. </p>
    </div>
  </div>
  <div>
    <video id="vidcmp_sparse_vid0" controls loop class="on">
      <source src="videos/llff_horns_ours.mp4" type="video/mp4"/>
    </video>
    <video id="vidcmp_sparse_vid1" controls loop>
      <source src="videos/llff_horns_nerf.mp4" type="video/mp4"/>
    </video>
    <video id="vidcmp_sparse_vid2" controls loop>
      <source src="videos/llff_horns_ours_teacher.mp4" type="video/mp4"/>
    </video>
  </div>
</div>
<h2 id="vidcmp_dense">
  View Synthesis Videos from Dense Light Fields
  <span><a href="#top">&uarr;</a></span>
</h2>
<div class="flex">
  <div>
    <div class="flex">
      <div>
        <button id="vidcmp_dense_data0" value="stanford_treasure" class="on">Treasure</button>
        <button id="vidcmp_dense_data1" value="stanford_bulldozer">Bulldozer</button>
        <button id="vidcmp_dense_data2" value="stanford_tarot_small">Tarot (Small)</button>
      </div>
      <div class="wider_buttons">
        <button id="vidcmp_dense_btn0" value="gt">Nearest Ground Truth View</button>
        <button id="vidcmp_dense_btn1" value="ours" class="on">Our Result</button>
        <button id="vidcmp_dense_btn2" value="nerf">NeRF</button>
        <button id="vidcmp_dense_btn3" value="xfields">X-Fields</button>
      </div>
    </div>
    <div class="desc">
      <p>We show spiral or linear trajectories for a select few scenes.</p>
      <p><b>Datasets.</b> We use 3 scenes from the Stanford Light Fields dataset&mdash;Treasure, Bulldozer, and Tarot (Small).</p>
      <p><b>Baselines.</b> As noted previously, we show better sharpness and reproduction of view dependence than both baselines. Specifically, note the reflections on gems in Treasure, the reflections on the bulldozer arm in Bulldozer, and the refractions within the glass ball in Tarot (Small). </p>
    </div>
  </div>
  <div>
    <video id="vidcmp_dense_vid0" controls loop>
      <source src="videos/stanford_treasure_gt.mp4" type="video/mp4"/>
    </video>
    <video id="vidcmp_dense_vid1" controls loop class="on">
      <source src="videos/stanford_treasure_ours.mp4" type="video/mp4"/>
    </video>
    <video id="vidcmp_dense_vid2" controls loop>
      <source src="videos/stanford_treasure_nerf.mp4" type="video/mp4"/>
    </video>
    <video id="vidcmp_dense_vid3" controls loop>
      <source src="videos/stanford_treasure_xfields.mp4" type="video/mp4"/>
    </video>
  </div>
</div>
<h2 id="vidcmp_embed">
  Embedding Comparison for Dense Light Fields
  <span><a href="#top">&uarr;</a></span>
</h2>
<div class="flex">
  <div>
    <div class="flex">
      <div>
        <button id="vidcmp_embed_data0" value="stanford_bulldozer" class="on">Bulldozer</button>
      </div>
      <div class="wider_buttons">
        <button id="vidcmp_embed_btn0" value="gt">Nearest Ground Truth View</button>
        <button id="vidcmp_embed_btn1" value="ours_no_warp">No Embedding</button>
        <button id="vidcmp_embed_btn2" value="ours_feature">Feature Embedding</button>
        <button id="vidcmp_embed_btn3" value="ours" class="on">Local Affine-Transformation Embedding</button>
      </div>
    </div>
    <div class="desc">
      <p>We compare our embedding approaches in video form for a single dense scene.</p>
      <p><b>Datasets.</b> We use the Bulldozer scene from the Stanford Light Fields dataset.</p>
      <p><b>Baselines.</b> Our method without embedding leads to predictions that are severely distorted, and that do not maintain multi-view consistency. Our feature embedding approach exhibits low frequency "wobbling" artifacts that are obvious when the video is viewed in full screen. Our local affine-transformation embedding approach leads to the best overall quantitative performance, and multi-view consistency. </p>
    </div>
  </div>
  <div>
    <video id="vidcmp_embed_vid0" controls loop>
      <source src="videos/stanford_bulldozer_gt.mp4" type="video/mp4"/>
    </video>
    <video id="vidcmp_embed_vid1" controls loop>
      <source src="videos/stanford_bulldozer_ours_no_warp.mp4" type="video/mp4"/>
    </video>
    <video id="vidcmp_embed_vid2" controls loop>
      <source src="videos/stanford_bulldozer_ours_feature.mp4" type="video/mp4"/>
    </video>
    <video id="vidcmp_embed_vid3" controls loop class="on">
      <source src="videos/stanford_bulldozer_ours.mp4" type="video/mp4"/>
    </video>
  </div>
</div>
</body>
</html>

