<!doctype html>
<html>
<head>
<title>Learning Neural Light Fields with Ray-Space Embedding Networks</title>
<link href="resources/style.css" rel="stylesheet"/>
<script src="resources/handlers.js"></script>
<script>
window.onload = function() {
  teaser_sparse_handler = new VideoHandler(1, 3, "teaser_sparse_data", "teaser_sparse_btn", "teaser_sparse_vid");
  teaser_sparse_handler.register();
  teaser_sparse_handler.play_video();

  teaser_dense_handler = new VideoHandler(1, 4, "teaser_dense_data", "teaser_dense_btn", "teaser_dense_vid");
  teaser_dense_handler.register();
  teaser_dense_handler.play_video();
};
</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CQVT3LMJLZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-CQVT3LMJLZ');
</script>
</head>
<body>
<h1 class="center">Learning Neural Light Fields with Ray-Space Embedding Networks</h1>
<div class="authors">
  <div>
    <p><a href="https://www.battal.me/">Benjamin Attal</a></p>
    <p>Carnegie Mellon University</p>
  </div>
  <div>
    <p><a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a></p>
    <p>Meta</p>
  </div>
  <div>
    <p><a href="https://zollhoefer.com/">Michael Zollh&ouml;fer</a></p>
    <p>Reality Labs Research</p>
  </div>
  <div>
    <p><a href="https://johanneskopf.de/">Johannes Kopf</a></p>
    <p>Meta</p>
  </div>
  <div>
    <p><a href="https://changilkim.com/">Changil Kim</a></p>
    <p>Meta</p>
  </div>
</div>

<!-- links -->
<div class="links">
  <a href="https://arxiv.org/abs/2112.01523"><span><img src="resources/file-earmark-pdf-fill.svg"/> Paper (arXiv)</span></a>
  <a href="results.html"><span><img src="resources/file-earmark-code-fill.svg"/> Supplemental Results</span></a>
  <a href="https://www.youtube.com/watch?v=9JxqkGZwJ5w"><span><img src="resources/youtube.svg" style="height: 1.2em; vertical-align: -20%;"/> Overview Video</span></a>
  <a href="https://github.com/facebookresearch/neural-light-fields"><span><img src="resources/github.svg"/> Code</span></a>
  <a href="https://drive.google.com/drive/folders/1PHIFKF-KFllbyQACRf4-CBolQOM5-Fvk?usp=sharing"><span><img src="resources/hdd-fill.svg" style="vertical-align: -10%;"/> All Results</span></a>
  <a href="https://drive.google.com/drive/folders/1MnniS2uk5vIQ4XtzVSbD-CY7oEZQoL-2?usp=sharing"><span><img src="resources/hdd-fill.svg" style="vertical-align: -10%;"/> Data</span></a>
</div>

<!-- teaser close-ups -->
<div class="flex" style="justify-content: space-around; margin-bottom: 1em;">

  <div class="flex" style="width: 410px; justify-content: space-between;">
    <div class="closeup">
      <img src="figures/horns/closeup1_gt.png">
      <div><em>Ground Truth</em></div>
    </div>
    <div class="closeup">
      <img src="figures/horns/closeup1_ours.png">
      <div><em>Ours</em><br/>31.7dB / 2.1s</div>
    </div>
    <div class="closeup">
      <img src="figures/horns/closeup1_nerf.png">
      <div><a href="https://www.matthewtancik.com/nerf"><em>NeRF</em></a><br/>31.1dB / 10.4s</div>
    </div>
    <div class="closeup">
      <img src="figures/horns/closeup1_ours_teacher.png">
      <div><span class="tooltip"><em>Ours (w/ t)</em><span class="tooltiptext">Ours with soft student-teacher regularization with a trained NeRF as teacher.</span></span><br/>31.7dB / 2.1s</div>
    </div>
  </div>

  <div class="flex" style="width: 410px; justify-content: space-between;">
    <div class="closeup">
      <img src="figures/tarot_small/closeup1_gt.png">
      <div><em>Ground Truth</em></div>
    </div>
    <div class="closeup">
      <img src="figures/tarot_small/closeup1_ours.png">
      <div><em>Ours</em><br/>36.8dB / 0.11s</div>
    </div>
    <div class="closeup">
      <img src="figures/tarot_small/closeup1_nerf.png">
      <div><a href="https://www.matthewtancik.com/nerf"><em>NeRF</em></a><br/>35.4dB / 13.7s</div>
    </div>
    <div class="closeup">
      <img src="figures/tarot_small/closeup1_xfields.png">
      <div><a href="https://xfields.mpi-inf.mpg.de/"><em>X-Fields</em></a><br/>33.0dB / 0.05s</div>
    </div>
  </div>

</div>

<!-- teaser videos -->
<div class="flex">

  <div class="teaser">
    <div class="crop horns" style="position: relative;">
      <div style="width: 50px; height: 50px; position: absolute; left: 90px; top: 0px; border-radius: 10px; border: 2px solid red;"></div>
      <div style="width: 50px; height: 50px; position: absolute; right: 40px; bottom: 70px; border-radius: 10px; border: 2px dotted red;"></div>
      <video id="teaser_sparse_vid0" loop class="on">
        <source src="videos/llff_horns_ours.mp4" type="video/mp4"/>
      </video>
      <video id="teaser_sparse_vid1" loop>
        <source src="videos/llff_horns_nerf.mp4" type="video/mp4"/>
      </video>
      <video id="teaser_sparse_vid2" loop>
        <source src="videos/llff_horns_ours_teacher.mp4" type="video/mp4"/>
      </video>
    </div>
    <div style="display: none;">
      <button id="teaser_sparse_data0" value="llff_horns">Horns</button>
    </div>
    <div class="horns">
      <div class="flex">
        <button id="teaser_sparse_btn0" value="ours" class="on">Ours</button>
        <button id="teaser_sparse_btn1" value="nerf">NeRF</button>
        <button id="teaser_sparse_btn2" value="ours_teacher">Ours (w/ Teacher)</button>
      </div>
    </div>
  </div>

  <div class="teaser">
    <div class="crop tarot" style="position: relative;">
      <!--
      <div style="width: 80px; height: 80px; position: absolute; left: 150px; top: 90px; border-radius: 10px; border: 2px solid red;"></div>
      -->
      <div style="width: 70px; height: 70px; position: absolute; left: 155px; top: 95px; border-radius: 10px; border: 2px solid red;"></div>
      <div style="width: 80px; height: 30px; position: absolute; left: 180px; top: 80px; border-radius: 10px; border: 2px dotted red;"></div>
      <video id="teaser_dense_vid0" loop>
        <source src="videos/stanford_tarot_small_gt.mp4" type="video/mp4"/>
      </video>
      <video id="teaser_dense_vid1" loop class="on">
        <source src="videos/stanford_tarot_small_ours.mp4" type="video/mp4"/>
      </video>
      <video id="teaser_dense_vid2" loop>
        <source src="videos/stanford_tarot_small_nerf.mp4" type="video/mp4"/>
      </video>
      <video id="teaser_dense_vid3" loop>
        <source src="videos/stanford_tarot_small_xfields.mp4" type="video/mp4"/>
      </video>
    </div>
    <div style="display: none;">
      <button id="teaser_dense_data0" value="stanford_tarot_small">Tarot (Small)</button>
    </div>
    <div class="tarot">
      <div class="flex">
        <button id="teaser_dense_btn0" value="gt">Nearest</button>
        <button id="teaser_dense_btn1" value="ours" class="on">Ours</button>
        <button id="teaser_dense_btn2" value="nerf">NeRF</button>
        <button id="teaser_dense_btn3" value="xfields">X-Fields</button>
      </div>
    </div>
  </div>

</div>

<h2>Overview</h2>
<div class="justify">
  <p>We present <em>Neural Light Fields with Ray-Space Embedding</em>. A light field directly represents integrated radiance along rays. Unlike neural radiance fields, which need many network evaluations to approximate a volume integral, rendering from a light field only requires one evaluation per ray.</p>
  <img src="figures/web_teaser.svg" style="display: block; width: 80%; margin-left: auto; margin-right: auto;"/>
  <p>However, learning a neural light field is challenging, and using popular coordinate-based neural network architectures leads to poor view synthesis quality. We present a novel ray-space embedding approach to mitigate this challenge. By leveraging ray-space embedding, in addition to subdivision, and soft student-teacher training, our neural light fields provide a more favorable trade-off between quality, speed, and memory than the previous state of the art, as shown in the graph below.</p>
  <img src="figures/teaser_graph.svg" style="display: block; width: 40%; margin-left: auto; margin-right: auto;"/>
  <p>We demonstrate our method on a variety of scenes, both sparsely and densely sampled, from the <a href="https://www.matthewtancik.com/nerf">NeRF's Real Forward-Facing</a> dataset and the <a href="http://lightfield.stanford.edu/">Stanford Light Fields</a> dataset. More results, including baseline comparisons and ablations, are found on <a href="results.html">our supplemental webpage</a>.
</div>

<!--
<div style="text-align: center;">
  <div class="flex" style="justify-content: space-between; align-items: center; margin-top: 2em;">
    <img src="figures/web_teaser.svg" style="display: block; width: 65%; margin-left:"/>
    <img src="figures/teaser_graph.svg" style="display: block; width: 34%; margin-left:"/>
  </div>
  <p>We present <em>Neural Light Fields with Ray-Space Embedding</em>, which map rays directly to integrated radiance.</p>
</div>
-->

<!--
<div class="flex" style="justify-content: space-between; align-items: center; margin-top: 2em;">
  <img src="figures/teaser_illustration.svg" style="width: 29%;"/>
  <img src="figures/teaser_visual.svg" style="width: 40%;"/>
  <img src="figures/teaser_graph.svg" style="width: 29%;"/>
</div>
<p class="justify">We present <em>Neural Light Fields with Ray-Space Embedding</em>, which map rays directly to integrated radiance (<em>left</em>). Our representation allows for high-quality view synthesis with faithful reconstruction of complex view dependence (<em>middle</em>). We are able to synthesize novel views in a fraction of the time required for the current state-of-the-art approaches, with comparable memory footprint. Our method achieves a more favorable trade-off between quality, speed, and memory than existing approaches for both dense and sparse light fields (<em>right</em>).</p>
-->
<h2>Abstract</h2>
<p class="justify">Neural radiance fields (NeRFs) produce state-of-the-art view synthesis results. However, they are slow to render, requiring hundreds of network evaluations per pixel to approximate a volume rendering integral. Baking NeRFs into explicit data structures enables efficient rendering, but results in a large increase in memory footprint and, in many cases, a quality reduction. In this paper, we propose a novel neural light field representation that, in contrast, is compact and directly predicts integrated radiance along rays. Our method supports rendering with a single network evaluation per pixel for small baseline light field datasets and can also be applied to larger baselines with only a few evaluations per pixel. At the core of our approach is a ray-space embedding network that maps the 4D ray-space manifold into an intermediate, interpolable latent space. Our method achieves state-of-the-art quality on dense forward-facing datasets such as the Stanford Light Field dataset. In addition, for forward-facing scenes with sparser inputs we achieve results that are competitive with NeRF-based approaches in terms of quality while providing a better speed/quality/memory trade-off with far fewer network evaluations.</p>
<h2>Overview Video</h2>
<div style="margin-left: auto; margin-right: auto; text-align: center;">
  <iframe width="840" height="472" src="https://www.youtube.com/embed/9JxqkGZwJ5w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<h2>BibTeX</h2>
<pre><code>@inproceedings{attal2022learning,
  author    = {Benjamin Attal and Jia-Bin Huang and Michael Zollh{\"o}fer and Johannes Kopf and Changil Kim},
  title     = {Learning Neural Light Fields with Ray-Space Embedding Networks},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2022},
}</code></pre>
</body>
</html>
